{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlp_to_qbaf_converter.mlp_to_qbaf import MLPToQBAF\n",
    "from sparx.sparx import LocalSpArX\n",
    "import Uncertainpy.src.uncertainpy.gradual as grad\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from mlp_to_qbaf_converter.counterfactual_contestability_explanation import CounterfactualContestabilityExplanation\n",
    "from mlp_to_qbaf_converter.utils import forward_pass_dataset, logistic\n",
    "from scipy.optimize import minimize, differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2025\n",
    "data = pd.read_csv(\"../../data/diabetes.csv\")\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "input_feature_names = list(X.columns)\n",
    "output_names = [\"Diabetes?\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    X_train.to_numpy(),\n",
    "    X_test.to_numpy(),\n",
    "    y_train.to_numpy(),\n",
    "    y_test.to_numpy(),\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=seed, sampling_strategy=\"minority\")\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling (Use StandardScaler instead of MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.1s\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.1s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.3s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s\n",
      "\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "size = (5,)\n",
    "min_accuracy = 0.7\n",
    "\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "solvers = [\"adam\", \"sgd\", \"lbfgs\"]\n",
    "\n",
    "best_mlp = None\n",
    "best_score = 0\n",
    "params = {\"alpha\": alphas, \"solver\": solvers}\n",
    "grid_search = GridSearchCV(\n",
    "    MLPClassifier(\n",
    "        hidden_layer_sizes=size,\n",
    "        activation=\"logistic\",\n",
    "        random_state=seed,\n",
    "        max_iter=5000,\n",
    "        early_stopping=True,\n",
    "    ),\n",
    "    params,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_mlp = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "if best_score < min_accuracy:\n",
    "    msg = \"Model accuracy is too low\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "mlp = best_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78       141\n",
      "           1       0.64      0.74      0.69        90\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.73      0.74      0.73       231\n",
      "weighted avg       0.75      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{l}\n",
    "\\operatorname{Agg}_{x}^{e}\\left(C_{1}, C_{2}\\right)=\n",
    "\\quad \\sum_{x^{\\prime} \\in \\Delta^{\\prime}} \\pi_{x^{\\prime}, x} \\sum_{v_{l, i} \\in C_{1}} \\frac{1}{\\left|C_{2}\\right| \\cdot \\mathcal{O}_{x^{\\prime}}^{\\mu}\\left(v_{C_{1}}\\right)} \\sum_{v_{l+1, j} \\in C_{2}} W_{i, j}^{l} \\mathcal{O}_{x^{\\prime}}^{\\mathcal{M}}\\left(v_{l, i}\\right)\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weights(partial_weights, n_clusters, layer, cluster_labels):\n",
    "\n",
    "    merged_weights = [np.mean(partial_weights.T[cluster_labels[layer] == label], axis=0) for label in range(n_clusters)]\n",
    "\n",
    "    return np.asarray(merged_weights).T\n",
    "\n",
    "def update_partial_weights(W, layer, sp, merged_weights, merged_biases, n_clusters, activations_dataset):\n",
    "    new_partial_weights = []\n",
    "    partial_activations = forward_pass_dataset(\n",
    "        sp.local_dataset[:, :-1],\n",
    "        merged_weights, \n",
    "        merged_biases,\n",
    "        logistic\n",
    "    )[layer + 1]\n",
    "\n",
    "    new_example_weights = sp.example_weights / np.sum(sp.example_weights)\n",
    "\n",
    "    for label in range(n_clusters):\n",
    "        h_star = partial_activations[:, label]\n",
    "        all_hidden_activations = activations_dataset[layer + 1][\n",
    "            :,\n",
    "            sp.cluster_labels[layer] == label,\n",
    "        ]\n",
    "\n",
    "        h_star = np.array([1 if hs == 0 else hs for hs in list(h_star)])\n",
    "\n",
    "        normalised_activations = np.sum(\n",
    "            np.multiply(all_hidden_activations.T, new_example_weights / h_star).T,\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        new_partial_weights.append(\n",
    "            np.dot(\n",
    "                normalised_activations,\n",
    "                W[sp.cluster_labels[layer] == label],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return np.asarray(new_partial_weights)\n",
    "\n",
    "def merge_layer(W, shape, layer, n_clusters, sp: LocalSpArX, merged_weights, merged_biases, activations_dataset, num_hidden_layers, sparse_weights):\n",
    "\n",
    "    W = W.reshape(shape)\n",
    "    \n",
    "    if layer == 0:\n",
    "        return np.linalg.norm(merge_weights(W, n_clusters, layer, sp.cluster_labels) - sparse_weights[layer])\n",
    "    \n",
    "    new_partial_weights = update_partial_weights(\n",
    "        W, \n",
    "        layer - 1,\n",
    "        sp,\n",
    "        merged_weights,\n",
    "        merged_biases,\n",
    "        n_clusters,\n",
    "        activations_dataset\n",
    "    )\n",
    "\n",
    "    if layer == num_hidden_layers:\n",
    "        return np.linalg.norm(new_partial_weights - sparse_weights[layer])\n",
    "\n",
    "    return np.linalg.norm(merge_weights(new_partial_weights, n_clusters, layer, sp.cluster_labels) - sparse_weights[layer])\n",
    "    \n",
    "\n",
    "def compute_new_weights(sp: LocalSpArX, original_weights, original_biases, sparse_weights, sparse_biases):\n",
    "\n",
    "    computed_weights = []\n",
    "\n",
    "    num_hidden_layers = len(sp.sparsified_weights) - 1\n",
    "\n",
    "    activations_dataset = []\n",
    "        \n",
    "\n",
    "    for i in range(num_hidden_layers + 1):\n",
    "        print(f\"{i}/{num_hidden_layers}\")\n",
    "        n_clusters = max(sp.cluster_labels[i]) + 1\n",
    "        W_init = original_weights[i]\n",
    "        shape = W_init.shape\n",
    "        W_init = W_init.flatten()\n",
    "        bounds = [(-1, 1) for _ in W_init]\n",
    "        sol = minimize(merge_layer, W_init, args=(shape, i, n_clusters, sp, sparse_weights[:i], sparse_biases[:i], activations_dataset, num_hidden_layers, sparse_weights), bounds=bounds)\n",
    "        computed_weights.append(sol.x.reshape(shape))\n",
    "        activations_dataset = forward_pass_dataset(\n",
    "            sp.local_dataset[:, :-1],\n",
    "            computed_weights,\n",
    "            original_biases[:i+1],\n",
    "            logistic,\n",
    "        )\n",
    "    \n",
    "    return computed_weights\n",
    "\n",
    "def create_new_qbaf(  # noqa: PLR0913\n",
    "    original_qbaf: grad.BAG,\n",
    "    original_mlp: tuple[np.ndarray, np.ndarray],\n",
    "    sparse_qbaf: grad.BAG,\n",
    "    sp: LocalSpArX,\n",
    "    counterfactual_dict: dict[tuple[str, str], float],\n",
    "    input_feature_names: list[str],\n",
    "    output_names: list[str],\n",
    "    example: np.ndarray,\n",
    ") -> grad.BAG:\n",
    "    \"\"\"Calculate the change in weights for the original QBAF.\n",
    "\n",
    "    Based on the difference in weights between the sparse and its counterfactual.\n",
    "\n",
    "    :param original_qbaf: The original QBAF.\n",
    "    :param original_mlp: The original MLP weights and biases.\n",
    "    :param sparse_qbaf: The sparse QBAF.\n",
    "    :param sp: The LocalSpArX object.\n",
    "    :param counterfactual_dict: The weights for the sparse QBAF counterfactual.\n",
    "    :param input_feature_names: The names of the input features.\n",
    "    :param output_names: The names of the output features.\n",
    "    :param example: The example to explain.\n",
    "\n",
    "    :return: A dictionary containing the change in weights for the original QBAF.\n",
    "\n",
    "    \"\"\"\n",
    "    original_weights, original_biases = original_mlp\n",
    "    sparse_weights, sparse_biases = sp.get_sparsified_mlp()\n",
    "\n",
    "    for relation in counterfactual_dict:\n",
    "        inital = relation[0]\n",
    "        final = relation[1]\n",
    "\n",
    "        if sparse_qbaf.arguments[inital] in sparse_qbaf.arguments[final].attackers:\n",
    "            attack = True\n",
    "        else:\n",
    "            attack = False\n",
    "\n",
    "        if inital in input_feature_names:\n",
    "            inital_layer = 0\n",
    "            inital_neuron = input_feature_names.index(inital)\n",
    "        else:\n",
    "            split_name = inital.split(\" \")\n",
    "            inital_layer = int(split_name[1])\n",
    "            inital_neuron = int(split_name[3]) - 1\n",
    "        if final in output_names:\n",
    "            final_neuron = output_names.index(final)\n",
    "        else:\n",
    "            split_name = final.split(\" \")\n",
    "            final_neuron = int(split_name[3]) - 1\n",
    "\n",
    "        if attack:\n",
    "            sparse_weights[inital_layer][inital_neuron][final_neuron] = -counterfactual_dict[relation]\n",
    "        else:\n",
    "            sparse_weights[inital_layer][inital_neuron][final_neuron] = counterfactual_dict[relation]\n",
    "\n",
    "    new_weights = compute_new_weights(sp, original_weights, original_biases, sparse_weights, sparse_biases)\n",
    "    \n",
    "\n",
    "    neurons_per_layer = [len(input_feature_names)] + [len(bias) for bias in original_biases]\n",
    "    new_qbaf = MLPToQBAF(\n",
    "        neurons_per_layer,\n",
    "        new_weights,\n",
    "        original_biases,\n",
    "        \"logistic\",\n",
    "        input_feature_names,\n",
    "        output_names,\n",
    "        example\n",
    "    ).get_qbaf()\n",
    "\n",
    "    counterfactual_qbaf = MLPToQBAF(\n",
    "        sp.get_sparsified_shape(),\n",
    "        sparse_weights,\n",
    "        sparse_biases,\n",
    "        \"logistic\",\n",
    "        input_feature_names,\n",
    "        output_names,\n",
    "        example\n",
    "    ).get_qbaf()\n",
    "\n",
    "    return new_qbaf, counterfactual_qbaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(\n",
    "    new_qbaf: grad.BAG,\n",
    "    desired_strength: float,\n",
    "    delta: float,\n",
    "    topic_arg_name: str,\n",
    ") -> tuple[bool, float]:\n",
    "    \"\"\"Check if the new QBAF is valid based on the desired strength.\"\"\"\n",
    "    # Get the strength of the topic argument\n",
    "    topic_arg = new_qbaf.arguments[topic_arg_name]\n",
    "    topic_strength = topic_arg.strength\n",
    "\n",
    "    # Check if the strength is greater than or equal to the desired strength\n",
    "    dist = abs(desired_strength - topic_strength)\n",
    "    return (dist < delta, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: FIX CE class to given assign functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterfactual sparse QBAF: (True, 0.009620286010456891)\n",
      "0/1\n",
      "1/1\n",
      "Function counterfactual sparse QBAF: (True, 0.009620286010456891)\n",
      "Validity: False\n",
      "Distance: 0.10772967268538941\n",
      "Desired strength: 0.27409607462665897\n",
      "Original Strength: 0.0852817418684515\n",
      "New Strength: 0.16636640194126956\n"
     ]
    }
   ],
   "source": [
    "sparsification = 10\n",
    "\n",
    "X_test = np.clip(X_test, 0, 1)\n",
    "\n",
    "neurons_per_layer = [\n",
    "    mlp.n_features_in_,\n",
    "    *list(mlp.hidden_layer_sizes),\n",
    "    mlp.n_outputs_,\n",
    "]\n",
    "train_set = np.column_stack((X_train, y_train))\n",
    "\n",
    "example_num = 50\n",
    "\n",
    "example = X_test[example_num]\n",
    "example_row = np.append(X_test[example_num], y_test[example_num])\n",
    "\n",
    "original_qbaf = MLPToQBAF(\n",
    "    neurons_per_layer,\n",
    "    mlp.coefs_,\n",
    "    mlp.intercepts_,\n",
    "    \"logistic\",\n",
    "    input_feature_names,\n",
    "    output_names,\n",
    "    example,\n",
    ").get_qbaf()\n",
    "\n",
    "sp = LocalSpArX(\n",
    "    mlp.coefs_,\n",
    "    mlp.intercepts_,\n",
    "    \"logistic\",\n",
    "    sparsification,\n",
    "    example_row,\n",
    "    train_set,\n",
    "    np.sqrt(X_test.shape[1]) * 0.75,\n",
    ")\n",
    "\n",
    "sp_weights, sp_biases = sp.get_sparsified_mlp()\n",
    "\n",
    "sparse_qbaf = MLPToQBAF(\n",
    "    sp.get_sparsified_shape(),\n",
    "    sp_weights,\n",
    "    sp_biases,\n",
    "    \"logistic\",\n",
    "    input_feature_names,\n",
    "    output_names,\n",
    "    example,\n",
    ").get_qbaf()\n",
    "\n",
    "cf_obj = CounterfactualContestabilityExplanation(\n",
    "    sparse_qbaf,\n",
    "    grad.SumAggregation(),\n",
    "    grad.MLPBasedInfluence(),\n",
    "    output_names[0],\n",
    "    seed,\n",
    ")\n",
    "\n",
    "sparse_counterfactual = cf_obj.get_ce_explanation()\n",
    "\n",
    "for relation, val in sparse_counterfactual.items():\n",
    "    inital = sparse_qbaf.arguments[relation[0]]\n",
    "    final = sparse_qbaf.arguments[relation[1]]\n",
    "\n",
    "    if inital in final.attackers:\n",
    "        final.attackers[inital] = val\n",
    "    else:\n",
    "        final.supporters[inital] = val\n",
    "    \n",
    "grad.computeStrengthValues(sparse_qbaf, grad.SumAggregation(), grad.MLPBasedInfluence())\n",
    "\n",
    "print(f\"Counterfactual sparse QBAF: {check_validity(sparse_qbaf, cf_obj.desired_strength, cf_obj.delta, output_names[0])}\")\n",
    "\n",
    "    \n",
    "new_estimated_qbaf, counterfactual_qbaf = create_new_qbaf(\n",
    "    original_qbaf,\n",
    "    (mlp.coefs_, mlp.intercepts_),\n",
    "    sparse_qbaf,\n",
    "    sp,\n",
    "    sparse_counterfactual,\n",
    "    input_feature_names,\n",
    "    output_names,\n",
    "    example,\n",
    ")\n",
    "\n",
    "print(f\"Function counterfactual sparse QBAF: {check_validity(counterfactual_qbaf, cf_obj.desired_strength, cf_obj.delta, output_names[0])}\")\n",
    "\n",
    "is_valid, dist = check_validity(\n",
    "    new_estimated_qbaf,\n",
    "    cf_obj.desired_strength,\n",
    "    cf_obj.delta,\n",
    "    output_names[0],\n",
    ")\n",
    "\n",
    "print(f\"Validity: {is_valid}\")\n",
    "print(f\"Distance: {dist}\")\n",
    "print(f\"Desired strength: {cf_obj.desired_strength}\")\n",
    "print(f\"Original Strength: {original_qbaf.arguments[output_names[0]].strength}\")\n",
    "print(f\"New Strength: {new_estimated_qbaf.arguments[output_names[0]].strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlp_to_qbaf_converter.utils import forward_pass\n",
    "\n",
    "# def sparsify_and_compare(\n",
    "#         weights_guess,\n",
    "#         sizes,\n",
    "#         original_biases,\n",
    "#         sparse_activations_example,\n",
    "#         example,\n",
    "#         sparse_weights,\n",
    "#         sp,\n",
    "#         train_set,\n",
    "#     ):\n",
    "\n",
    "#     weights_guess_reshaped = []\n",
    "\n",
    "#     for size in sizes:\n",
    "#         weights_guess_reshaped.append(np.reshape(weights_guess[:np.prod(size)], size))\n",
    "#         weights_guess = weights_guess[np.prod(size):]\n",
    "    \n",
    "#     sparse_approx = LocalSpArX(\n",
    "#         weights_guess_reshaped,\n",
    "#         original_biases,\n",
    "#         \"logistic\",\n",
    "#         sp.shrinkage_percentage,\n",
    "#         sp.example_row,\n",
    "#         train_set,\n",
    "#         sp.kernel_width,\n",
    "#     )\n",
    "\n",
    "#     approx_sparse_weights, approx_sparse_biases = sparse_approx.get_sparsified_mlp()\n",
    "#     approx_sparse_activations_example = forward_pass(\n",
    "#         example,\n",
    "#         approx_sparse_weights,\n",
    "#         approx_sparse_biases,\n",
    "#         logistic,\n",
    "#     )\n",
    "    \n",
    "#     return np.linalg.norm(sparse_activations_example[-1] - approx_sparse_activations_example[-1]) \n",
    "        \n",
    "# def create_new_qbaf(\n",
    "#         original_mlp,\n",
    "#         sparse_qbaf,\n",
    "#         sp,\n",
    "#         counterfactual_dict,\n",
    "#         input_feature_names,\n",
    "#         output_names,\n",
    "#         example,\n",
    "#         train_set,\n",
    "#         ):\n",
    "    \n",
    "#     original_weights, original_biases = original_mlp\n",
    "\n",
    "#     sparse_weights, sparse_biases = sp.get_sparsified_mlp()\n",
    "    \n",
    "#     # Apply the counterfactual dict to get new sparse weights\n",
    "#     for relation in counterfactual_dict:\n",
    "#         inital = relation[0]\n",
    "#         final = relation[1]\n",
    "\n",
    "#         if sparse_qbaf.arguments[inital] in sparse_qbaf.arguments[final].attackers:\n",
    "#             attack = True\n",
    "#         else:\n",
    "#             attack = False\n",
    "\n",
    "#         if inital in input_feature_names:\n",
    "#             inital_layer = 0\n",
    "#             inital_neuron = input_feature_names.index(inital)\n",
    "#         else:\n",
    "#             split_name = inital.split(\" \")\n",
    "#             inital_layer = int(split_name[1])\n",
    "#             inital_neuron = int(split_name[3]) - 1\n",
    "#         if final in output_names:\n",
    "#             final_neuron = output_names.index(final)\n",
    "#         else:\n",
    "#             split_name = final.split(\" \")\n",
    "#             final_neuron = int(split_name[3]) - 1\n",
    "\n",
    "#         if attack:\n",
    "#             sparse_weights[inital_layer][inital_neuron][final_neuron] = -counterfactual_dict[relation]\n",
    "#         else:\n",
    "#             sparse_weights[inital_layer][inital_neuron][final_neuron] = counterfactual_dict[relation]\n",
    "    \n",
    "#     sparse_activations_example = forward_pass(\n",
    "#         example,\n",
    "#         sparse_weights,\n",
    "#         sparse_biases,\n",
    "#         logistic,        \n",
    "#     )\n",
    "\n",
    "#     weights_list = []\n",
    "#     sizes = []\n",
    "    \n",
    "#     for i in range(len(original_weights)):\n",
    "#         sizes.append(original_weights[i].shape)\n",
    "#         weights_list += list(original_weights[i].flatten())\n",
    "    \n",
    "#     sol = differential_evolution(\n",
    "#         sparsify_and_compare,\n",
    "#         args=(sizes, original_biases, sparse_activations_example, example, sparse_weights, sp, train_set),\n",
    "#         bounds=[(-1, 1) for _ in range(len(weights_list))],\n",
    "#         strategy=\"best1bin\",\n",
    "#         maxiter=1000,\n",
    "#         disp=True,\n",
    "#     )\n",
    "\n",
    "#     new_weights = sol.x\n",
    "\n",
    "#     new_weights_reshaped = []\n",
    "\n",
    "#     for size in sizes:\n",
    "#         new_weights_reshaped.append(np.reshape(new_weights[:np.prod(size)], size))\n",
    "#         new_weights = new_weights[np.prod(size):]\n",
    "\n",
    "#     neurons_per_layer = [len(input_feature_names)] + [len(bias) for bias in original_biases]\n",
    "    \n",
    "#     estimated_qbaf = MLPToQBAF(\n",
    "#         neurons_per_layer,\n",
    "#         new_weights_reshaped,\n",
    "#         original_biases,\n",
    "#         \"logistic\",\n",
    "#         input_feature_names,\n",
    "#         output_names,\n",
    "#         example,\n",
    "#     ).get_qbaf()\n",
    "\n",
    "#     counterfactual_qbaf = MLPToQBAF(\n",
    "#         sp.get_sparsified_shape(),\n",
    "#         sparse_weights,\n",
    "#         sparse_biases,\n",
    "#         \"logistic\",\n",
    "#         input_feature_names,\n",
    "#         output_names,\n",
    "#         example\n",
    "#     ).get_qbaf()\n",
    "        \n",
    "#     return estimated_qbaf, counterfactual_qbaf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.1s\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.1s\n",
      "[CV] END .............................alpha=0.1, solver=adam; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.1, solver=sgd; total time=   0.1s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.2, solver=adam; total time=   0.1s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.2, solver=sgd; total time=   0.1s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.3, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.3, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ..............................alpha=0.4, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.5, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.4, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.5, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.3, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.6, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.5, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.6, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.4, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.7, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.6, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END .............................alpha=0.7, solver=adam; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.8, solver=sgd; total time=   0.0s\n",
      "[CV] END .............................alpha=0.9, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ............................alpha=0.7, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................alpha=0.9, solver=sgd; total time=   0.0s\n",
      "[CV] END ............................alpha=0.9, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................alpha=0.8, solver=adam; total time=   0.0s\n",
      "[CV] END ............................alpha=0.8, solver=lbfgs; total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "seed = 2025\n",
    "data = pd.read_csv(\"../../data/diabetes.csv\")\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "input_feature_names = list(X.columns)\n",
    "output_names = [\"Diabetes?\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    X_train.to_numpy(),\n",
    "    X_test.to_numpy(),\n",
    "    y_train.to_numpy(),\n",
    "    y_test.to_numpy(),\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=seed, sampling_strategy=\"minority\")\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling (Use StandardScaler instead of MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "size = (10,)\n",
    "min_accuracy = 0.7\n",
    "\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "solvers = [\"adam\", \"sgd\", \"lbfgs\"]\n",
    "\n",
    "best_mlp = None\n",
    "best_score = 0\n",
    "params = {\"alpha\": alphas, \"solver\": solvers}\n",
    "grid_search = GridSearchCV(\n",
    "    MLPClassifier(\n",
    "        hidden_layer_sizes=size,\n",
    "        activation=\"logistic\",\n",
    "        random_state=seed,\n",
    "        max_iter=5000,\n",
    "        early_stopping=True,\n",
    "    ),\n",
    "    params,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_mlp = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "if best_score < min_accuracy:\n",
    "    msg = \"Model accuracy is too low\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "mlp = best_mlp\n",
    "\n",
    "\n",
    "sparsification = 10\n",
    "\n",
    "X_test = np.clip(X_test, 0, 1)\n",
    "\n",
    "neurons_per_layer = [\n",
    "    mlp.n_features_in_,\n",
    "    *list(mlp.hidden_layer_sizes),\n",
    "    mlp.n_outputs_,\n",
    "]\n",
    "train_set = np.column_stack((X_train, y_train))\n",
    "\n",
    "example_num = 50\n",
    "\n",
    "example = X_test[example_num]\n",
    "example_row = np.append(X_test[example_num], y_test[example_num])\n",
    "\n",
    "original_qbaf = MLPToQBAF(\n",
    "    neurons_per_layer,\n",
    "    mlp.coefs_,\n",
    "    mlp.intercepts_,\n",
    "    \"logistic\",\n",
    "    input_feature_names,\n",
    "    output_names,\n",
    "    example,\n",
    ").get_qbaf()\n",
    "\n",
    "sp = LocalSpArX(\n",
    "    mlp.coefs_,\n",
    "    mlp.intercepts_,\n",
    "    \"logistic\",\n",
    "    sparsification,\n",
    "    example_row,\n",
    "    train_set,\n",
    "    np.sqrt(X_test.shape[1]) * 0.75,\n",
    ")\n",
    "\n",
    "sp_weights, sp_biases = sp.get_sparsified_mlp()\n",
    "\n",
    "sparse_qbaf = MLPToQBAF(\n",
    "    sp.get_sparsified_shape(),\n",
    "    sp_weights,\n",
    "    sp_biases,\n",
    "    \"logistic\",\n",
    "    input_feature_names,\n",
    "    output_names,\n",
    "    example,\n",
    ").get_qbaf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.4936048257898619)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cf_obj_orig = CounterfactualContestabilityExplanation(\n",
    "    original_qbaf,\n",
    "    grad.SumAggregation(),\n",
    "    grad.MLPBasedInfluence(),\n",
    "    output_names[0],\n",
    "    seed,\n",
    ")\n",
    "\n",
    "orig_counterfactual = cf_obj_orig.get_ce_explanation()\n",
    "\n",
    "print(len(orig_counterfactual))\n",
    "\n",
    "np.mean(\n",
    "    [abs(val) for val in orig_counterfactual.values()]\n",
    ")  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.49876833311891816)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_obj_sparse = CounterfactualContestabilityExplanation(\n",
    "    sparse_qbaf,\n",
    "    grad.SumAggregation(),\n",
    "    grad.MLPBasedInfluence(),\n",
    "    output_names[0],\n",
    "    seed,\n",
    ")\n",
    "\n",
    "sparse_counterfactual = cf_obj_sparse.get_ce_explanation()\n",
    "\n",
    "print(len(sparse_counterfactual))\n",
    "\n",
    "np.mean(\n",
    "    [abs(val) for val in sparse_counterfactual.values()]\n",
    ")  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
